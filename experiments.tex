\documentclass[Main]{subfiles}
\begin{document}

The planning problem was approached from two different angles, that were intended to be merged later. The two approaches were partial order planning and hierarchical task network. "[!Ref:]" 


\subsection{POP}

\textbf{Experimenting with back-tracking}

It was attempted to implement a back-tracking algorithm ....

\todo{Emil}





\subsection{HTN}
As part of the development a simple HTN algorithm for solving levels were implemented for solving single agent scenarios.
The algorithm utilized a genetic algorithm to calculate an optimal order in which to solve the goals.
The motivation for using a genetic algorithm, is that the problem of finding an optimal route to move a set of boxes to a set of goals, can be seen as a traveling salesman problem which is an np-complete task that genetic algorithms can help generate a solution for.
Genetic algorithms cannot calculate the most optimal order, but it can come up with a relative good suboptimal solution in a timely manner. The genetic algorithm used for the HTN experiments is inspired by the implementation at \cite{genetic}.

The genetic algorithm basically works by creating a population which is a list of lists where the inner lists are random permutations of all goals to be completed.
Next the outer list is sorted using an evaluation function which calculates the travel time to complete a given permutation of goals.
The algorithm now mutates and shuffles some of the population by using parts from the best performing permutations.
This goes on until the algorithm doesn't seem to produce good results anymore.

The HTN implementation itself, works by taking an ordered set of goals to be completed as input.
The first goal is picked and the algorithm decomposes a complex action to move to the box, where only move actions are considered.
When primitive move actions to the goal has been generated, primitive actions to move the box to the goal are calculated, where all possible actions (move,push and pull) are considered in the progress.
The algorithm uses the A* algorithm with a relaxed search space in the process of decomposing complex actions into primitive goals.
The process can be seen as decomposing a complex action CompleteSubGoal, which is about getting a box to a goal, into complex actions MoveToBox and MoveBoxToGoal, where MoveToBox consist of the actions to move the agent to a given box and MoveBoxToGoal consists of actions to move a given box to a goal.

The algorithm starts out with a relaxed search space, as it only takes the first box for the first goal into account and deals the rest of the boxes as obstacles when solving for the first goal.
When a goal has been completed, the current box remains active and no longer acts as an obstacle so it can be manipulated in the process of solving the next complex actions for the other goals.
This reduces the branching factor when solving the individual goals in the beginning, but adds more and more branching factor towards the completion of the unified plan.

In the end, the HTN algorithm was discarded because the POP algorithm was performing a lot better and it was therefore chosen to focus on the POP algorithm.


\subsection{General}


\textbf{Experimenting with heuristics}

- Weighted heuristics to encourage specific moves

The heuristics for the full problem search is a simple heuristic calculating the Manhattan distance between the active elements of a node. The active elements being an agent, a goal and a box chosen to solve the goal with. 




\textbf{Experiments with search strategy}

\begin{enumerate}
    \item BFS 
    \item Multi queue BFS (Maybe IDA* or Iterative deepening search)
\end{enumerate}



Benchmarking the multi queue BFS against standard BFS:





\subsection{Preprocessing ----- "??????" }

Experimenting with preprocessing and ``All goals shortest path'' 
- Part of the POP algorithm

- POP run after each solved goal 
--> Better solution --> SO much slower


During development experiments with different strategies regarding preprocessing, or extra-processing providing extra information, were conducted. 

As for the goal ordering based on a relaxed problem search on the initial state, the information about the level at this state is very useful. It is, however, preferred to have the same updated information after each solved goal. The moving of one or more boxes, and the change of the agent's location, will most likely leave the goal sorting less than optimal. It could therefore be beneficial to update the goal sorting, and maybe even the choosing of boxes for goals. 

In order to update the information a relaxed problem search is done based on the state after each goal is solved. 


- Performance affected by number of goals and agents in level, if few, maybe beneficial, if many it could produce a lot of overhead.






\subsection{Results}




\end{document}