\documentclass[Main]{subfiles}
\begin{document}

\textbf{Optimization of implemented client}


- Ordering of goals and goals conflicting - maybe look for goals with most walls around (such as safe goals)

- Use partial plan as guideline for Heuristics

- Better subgoal decomposition

- "Better" back tracking

- Re-order goals?

- Multi-agent: Conflict handling

The current implementation lacks solid conflict handling when solving multi-agent levels. To make up for this, we propose to enhance the system with the methods described in \cite{pellier2007unified}.

In this paper, an algorithm is described for solving multi agent problems where a set of interdependent agents need to collaborate to solve a common goal.
The algorithm uses methodologies from POP, HTN and a communication scheme between the agents to ensure the unified planning is consistent.
The algorithm works by generating a partial plan, for each agent in isolation from the other agents, and utilizes a communication scheme between the agents to ensure consistency in the resulting unified plan.

The algorithm starts out by defining a set of open goals for the whole level to be completed and an empty set of \textit{threats}.
The open goals contains unassigned subgoals to be completed in order for the common goal to be completed.
The set of \textit{threats} contains conditions that needs to be resolved in order to be able to solve a specific goal.

Each agent is assigned to accomplish one of these subgoals by adding a \textit{complex action} to the agents plan (where a \textit{complex action} is a high level action which can be refined into \textit{primitive actions}).
After each of the agents has been assigned a \textit{complex action}, the agents will start to refine them into \textit{primitive actions}.
The agents take turn in refining on the first unrefined \textit{complex action} in their plan.

If a conflict arises in the plan, eg. the refinement of a \textit{complex action} fails, a new \textit{threat} is added to the set of \textit{threats}.
One of the agents now has to resolve this \textit{threat} and add a \textit{complex action} to do so to its plan.
If refining a \textit{complex action} fails and there is no way to solve the \textit{threat}, the algorithm must backtrack and try alternative refinements of the failing \textit{complex action}. If the alternative refinements also fails the algorithm must backtrack further back to a previous \textit{complex action}.

During the planning process the agents communicate about \textit{threats}, \textit{repairs}, partial plan completions and \textit{refutations}.
A \textit{repair} is a suggestion on how to \textit{repair} a given \textit{threat}.
A partial plan completion is the event that an agent has achieved a plan for its goals successfully.
A \textit{refutation} is a message from one agent to another where the sender detects a conflict with his current plan and another agents move and therefore refutes the offending action generated by the other agent.

An example of a \textit{threat} can be that one agent is standing in the way of another agent, or that a box is in the way of and help is needed to move it out of the way.
A \textit{repair} for a given \textit{threat} can for example be that an offending agent moves away from a given path, or moves a box away from a given path.
A \textit{refutation} can for example be that agent '0' refutes the current action of agent '1', because agent '1' is moving to the same place as agent '0'. In this case a solution could be to tell agent '1' to wait until agent '0' has passed.

Obviously, the algorithm will need a way to judge when an agent should wait or if an agent should interrupt its current \textit{complex action} in order to solve a \textit{threat}. These decisions are important, because a wrong decision could lead to a plan which is much longer than the optimal solution.
To remedy this, a set of social laws could be taken into account each time the algorithm needs to prioritize between the agents.
One rule could for example be, that the agent who is closest to completing his current goal gets first priority, and another rule could be that an agent who is carrying a box has higher priority than agents not carrying boxes.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.3\textwidth]{plan_collab.png}
	\caption{Agent 0 needs to pass agent 1 in order to complete his goal, and vice versa, and thus there is a conflict which needs to be resolved before any of the agents can complete their individual goals. In the visualized scenario, agent '0' adds a \textit{threat} to have a clear path and agent '1' resolves the \textit{threat} by moving out of the way.}
	\label{fig:plan_collab}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\textwidth]{unhtnpop.png}
	\caption{Example of a plan collaboration between agent 0 and 1 at \textit{complex action} level. The agents initially gets assigned to complete a goal and a \textit{complex action} to do so, is added to their individual plans.}
	\label{fig:htn_collab}
\end{figure}

An example of the algorithm in action is visualized in Figure \ref{fig:htn_collab} and \ref{fig:plan_collab}, where two agents are in conflict about solving their individual goals.

In the scenario, agent '0' first tries to refine its \textit{complex action} to complete goal 'a' but discovers that there is a conflict since agent '1' is standing in its way.
Agent '0' can thus report failure for its plan, and add a new \textit{threat} to the set of unaccomplished \textit{threats} for getting a clear path to its goal.
Next agent '1' picks the \textit{threat} from the set of unaccomplished \textit{threats} and adds a \textit{complex action} to the beginning of its own plan, and reports to agent '0' that a \textit{repair} for the \textit{threat} has been found.
Next, agent '0' will attempt to refine its plan again, but will get a \textit{refutation} from agent '1' since it is in the process of moving out of the way and still is blocking for agent '0'.
As part of the \textit{refutation}, agent '1' will command agent '0' to wait for one turn.
When agent '1' is out of the way, agent '0' can start refining its \textit{complex action} to complete goal 'a'.
Since agent '1' has completed its \textit{complex action} to move out of the way of agent '0', it will start to refine its next \textit{complex action} which is to complete goal 'b'.
However upon generating the first action for the refinement of the \textit{complex action}, agent '1' will get a \textit{refutation} from agent '0', and agent '0' will command agent '1' to wait.
This conflict will repeat until agent '0' is out of the way, and finally agent '1' can refine its \textit{complex action} to solve goal 'b'.

\todo[inline]{Communication}

\todo[inline]{Auction of 'unsolvable' or expensive goals \cite{VanderKrogt2005}}
\todo[inline]{Cyclic dependencies}
\todo[inline]{\cite{Nguyen2001}}


\todo[inline]{Pruning algorithm to limit branching factor [http://en.wikipedia.org/wiki/Pruning\_(decision\_trees)]}
\todo[inline]{concurrency/parallel}

\end{document}
